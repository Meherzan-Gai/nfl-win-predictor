{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a09c5002-e9c2-44b9-be74-f91235f8dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "442a4dc1-20d6-4286-9d49-bd24e1ad3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTeamUrls(stats_url):\n",
    "    start = time.time()\n",
    "    data = requests.get(stats_url)\n",
    "    delay = time.time() - start\n",
    "    time.sleep(random.randint(60, 80)*delay)\n",
    "\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    afc_standings = soup.select('table.stats_table')\n",
    "    afc_team_links = []\n",
    "    for tag in afc_standings:\n",
    "        links = tag.find_all('a')\n",
    "        afc_team_links.extend(links)\n",
    "    afc_team_links = [link.get(\"href\") for link in afc_team_links]\n",
    "    \n",
    "    \n",
    "    \n",
    "    nfc_standings = soup.select('#NFC')\n",
    "    nfc_team_links = []\n",
    "    for tag in nfc_standings:\n",
    "        links = tag.find_all('a')\n",
    "        nfc_team_links.extend(links)\n",
    "    nfc_team_links = [link.get(\"href\") for link in nfc_team_links]\n",
    "    team_links = afc_team_links + nfc_team_links\n",
    "    team_urls = [f\"https://pro-football-reference.com{link}\" for link in team_links]\n",
    "    return team_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65d31c41-c746-438c-97c4-c7f8e2598ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making pandas database out of Game Results and Basic Stats\n",
    "def makeTeamDb (team_url):\n",
    "    start = time.time()\n",
    "    data = requests.get(team_url)\n",
    "    delay = time.time() - start\n",
    "    time.sleep(random.randint(60, 80)*delay)\n",
    "    \n",
    "    matches = pd.read_html(StringIO(data.text), match = \"Schedule & Game Results Table\")[0]\n",
    "\n",
    "    #Sanitizing for teams that went to playoffs (removes playoff rows)\n",
    "    while len(matches) > 18:\n",
    "        print(len(matches))\n",
    "        matches = matches.drop([18])\n",
    "        matches = matches.reset_index(drop = 'true')\n",
    "\n",
    "    \n",
    "    #Adding names for unnamed columns manually\n",
    "    matches = matches.rename(columns={'Unnamed: 3_level_1' : 'Time', 'Unnamed: 4_level_1' : 'Game Link', 'Unnamed: 5_level_1' : 'Result',\n",
    "                                      'Unnamed: 8_level_1' : 'Home/Away'},)\n",
    "    \n",
    "    \n",
    "    #Renaming columns \n",
    "    new_columns = []\n",
    "    for column in matches.columns:\n",
    "        if column[0] == 'Offense':\n",
    "            new_columns.append('Off' + column[1])\n",
    "                \n",
    "        elif column[0] == 'Defense':\n",
    "            new_columns.append('Def' + column[1])\n",
    "    \n",
    "        elif column[0] == 'Expected Points':\n",
    "            new_columns.append('EP' + column[1][:3])\n",
    "    \n",
    "        elif column[0] == 'Score':\n",
    "            new_columns.append(column[1] + 'Sc')\n",
    "    \n",
    "        else:\n",
    "            new_columns.append(column[1])\n",
    "    \n",
    "    matches.columns = new_columns\n",
    "    \n",
    "    \n",
    "    #Fixing empty data values\n",
    "    matches['OT'] = matches['OT'].apply(lambda x : 'N' if pd.isna(x) or x=='N' else 'Y')\n",
    "    matches['Result'] = matches['Result'].apply(lambda x : '' if pd.isna(x) or x=='' else x)\n",
    "    matches['Rec'] = matches['Rec'].apply(lambda x : '' if pd.isna(x) or x=='' else x)\n",
    "    matches['Home/Away'] = matches['Home/Away'].apply(lambda x : 'H' if pd.isna(x) or x=='H' else 'A')\n",
    "    \n",
    "    #Fixing empty data numbers to 0.0\n",
    "    for column in matches.columns[13:]:\n",
    "        matches[column] = matches[column].apply(lambda x : '0.0' if pd.isna(x) or x=='0.0' else x)\n",
    "    \n",
    "    #Fixing links to more detailed game stats\n",
    "    data = requests.get(team_url)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    game_table = soup.select('#games')\n",
    "    game_links = []\n",
    "    for tag in game_table:\n",
    "        links = tag.find_all('a')\n",
    "        game_links.extend(links)\n",
    "    \n",
    "    game_links = [link.get(\"href\") for link in game_links]\n",
    "    game_links = [link for link in game_links if '/boxscores' in link]\n",
    "    game_links = [f\"https://pro-football-reference.com{link}\" for link in game_links]\n",
    "\n",
    "    #Drops bye week to ensure proper adding of game_links and to remove it from the dataframes\n",
    "    bye_week = 0\n",
    "    while (matches['Opp'][bye_week] != 'Bye Week'):\n",
    "        bye_week+=1\n",
    "\n",
    "    matches = matches.drop([bye_week])\n",
    "    matches = matches.reset_index(drop = 'true')\n",
    "\n",
    "\n",
    "    #Sanitizes game_links to ensure that playoff game links are removed as the dataframe won't contain playoff matches\n",
    "    game_links = game_links[:17]\n",
    "    matches['Game Link'] = game_links\n",
    "\n",
    "    \n",
    "    #Finding the team name and adding it to the table\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    team_name = soup.find_all('h1')[0].find_all('span')[1].text\n",
    "    matches['Team'] = team_name\n",
    "    \n",
    "    #Finally calculating and adding the year to the table\n",
    "    year = team_url.split('/')[-1].replace('.htm', '')\n",
    "    matches['Season'] = year\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e0cd9a7-0f95-45c7-9ed1-3a37fd28ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2024,2021, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7de0925-726d-495b-a337-964092e7335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pro-football-reference.com/teams/buf/2024.htm\n",
      "https://pro-football-reference.com/teams/mia/2024.htm\n",
      "https://pro-football-reference.com/teams/nyj/2024.htm\n",
      "https://pro-football-reference.com/teams/nwe/2024.htm\n",
      "https://pro-football-reference.com/teams/pit/2024.htm\n",
      "https://pro-football-reference.com/teams/rav/2024.htm\n",
      "https://pro-football-reference.com/teams/cin/2024.htm\n",
      "https://pro-football-reference.com/teams/cle/2024.htm\n",
      "https://pro-football-reference.com/teams/htx/2024.htm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m team_url \u001b[38;5;129;01min\u001b[39;00m team_urls:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(team_url) \u001b[38;5;66;03m#used to monitor how quickly scraping occurs\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m         db \u001b[38;5;241m=\u001b[39m \u001b[43mmakeTeamDb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         all_matches\u001b[38;5;241m.\u001b[39mappend(db)\n\u001b[1;32m     18\u001b[0m match_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_matches)\n",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m, in \u001b[0;36mmakeTeamDb\u001b[0;34m(team_url)\u001b[0m\n\u001b[1;32m      5\u001b[0m delay \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m      6\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m60\u001b[39m)\u001b[38;5;241m*\u001b[39mdelay)\n\u001b[0;32m----> 8\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSchedule & Game Results Table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Sanitizing for teams that went to playoffs (removes playoff rows)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m18\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Work/Projects/win_predictor/virtual_env/lib/python3.13/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1225\u001b[0m     [\n\u001b[1;32m   1226\u001b[0m         is_file_like(io),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     ]\n\u001b[1;32m   1231\u001b[0m ):\n\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Work/Projects/win_predictor/virtual_env/lib/python3.13/site-packages/pandas/io/html.py:1003\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[1;32m   1005\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[0;32m~/Documents/Work/Projects/win_predictor/virtual_env/lib/python3.13/site-packages/pandas/io/html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[1;32m    973\u001b[0m     io,\n\u001b[1;32m    974\u001b[0m     compiled_match,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m     storage_options,\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[0;32m~/Documents/Work/Projects/win_predictor/virtual_env/lib/python3.13/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m~/Documents/Work/Projects/win_predictor/virtual_env/lib/python3.13/site-packages/pandas/io/html.py:598\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[0;34m(self, document, match, attrs)\u001b[0m\n\u001b[1;32m    596\u001b[0m tables \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mfind_all(element_name, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tables found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    601\u001b[0m unique_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "all_matches = [] #list of dataframes of all the game data for each year for each team\n",
    "\n",
    "for year in years: \n",
    "    stats_url = 'https://www.pro-football-reference.com/years/' + str(year) + '/index.htm' #sets the url to the current year's url (updates as the year changes)\n",
    "    \n",
    "    time.sleep(random.randint(60,80)) #extra sleep in between years to be safe\n",
    "    \n",
    "    team_urls = getTeamUrls(stats_url) #gets all the team urls from the page\n",
    "\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        print(team_url) #used to monitor how quickly scraping occurs\n",
    "        db = makeTeamDb(team_url)\n",
    "        all_matches.append(db)\n",
    "\n",
    "\n",
    "\n",
    "match_df = pd.concat(all_matches)\n",
    "match_df.to_csv(\"matches.csv\")\n",
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02d8414-1e29-4306-a29b-e06989af0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to save/cache all the html urls to this project in the html folder in case website goes down\n",
    "\n",
    "#team_urls = getTeamUrls(stats_url)\n",
    "#team_urls = ['https://www.pro-football-reference.com/teams/cle/2024.htm']\n",
    "\n",
    "\n",
    "#for team_url in team_urls:\n",
    "    #start = time.time()\n",
    "    #data = requests.get(team_url)\n",
    "    #delay = time.time() - start\n",
    "    #time.sleep(random.randint(20, 60)*delay)\n",
    "    #team_abr = team_url.split('/')[-2]\n",
    "    #year = team_url.split('/')[-1].replace('.htm', '')[-2:]\n",
    "    #with open ('./htmls/'+ team_abr + year + '.txt', 'w') as f:\n",
    "        #f.write(data.text)\n",
    "\n",
    "#with open ('./htmls/' + team_abr + '.txt', 'w') as f:\n",
    "    #f.write(\"Goodbye\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "virtual_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
